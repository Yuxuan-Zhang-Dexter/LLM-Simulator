{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Literal, Tuple, Dict\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Available Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load Models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current Available Device: {device}\")\n",
    "use_amp = amp_supported = torch.cuda.is_available() and torch.cuda.get_device_capability(0) >= (7, 0)\n",
    "tinyllama_model_name = \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "llama7b_model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "# local_model_dir = './model_cache'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n",
    "# Load model directly\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\",   torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Memory and Max Memory Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(\"Tinyllama/Tinyllama-1.1B-step-50K-105b\")\n",
    "\n",
    "# hidden_size = config.hidden_size\n",
    "# eos_token_id = config.eos_token_id\n",
    "# max_position_embeddings = config.max_position_embeddings\n",
    "\n",
    "# print(\"Hidden Size:\", hidden_size)\n",
    "# print(\"EOS Token ID:\", eos_token_id)\n",
    "# print(\"Max Position Embeddings:\", max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "b = 1 # batch size\n",
    "s = 1 # sequence length\n",
    "max_s = 4096 # max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelMemoryUtilities:\n",
    "    @staticmethod\n",
    "    def convert_memory(memory: int, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte') -> float:\n",
    "        \"\"\"Convert memory to the specified unit.\"\"\"\n",
    "        if memory_unit == 'mb':\n",
    "            return memory / 1048576  # Convert bytes to MB\n",
    "        elif memory_unit == 'gb':\n",
    "            return memory / 1073741824  # Convert bytes to GB\n",
    "        return memory  # Default to bytes\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logits(model_output) -> torch.Tensor:\n",
    "        \"\"\"Extract logits from model output, supporting various output formats.\"\"\"\n",
    "        if hasattr(model_output, 'logits'):\n",
    "            return model_output.logits\n",
    "        elif isinstance(model_output, torch.Tensor):\n",
    "            return model_output\n",
    "        else:\n",
    "            raise ValueError(\"Model output does not contain logits or is not a tensor.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def measure_cpu_memory():\n",
    "        \"\"\"Measure CPU memory usage in bytes.\"\"\"\n",
    "        return psutil.Process().memory_info().rss\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_memory_lines(prev_memory: int, cur_memory_lst: list, peak_memory_lst: list = None, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "        \"\"\"Plot memory usage over iterations.\"\"\"\n",
    "        conv_prev_memory = ModelMemoryUtilities.convert_memory(prev_memory, memory_unit)\n",
    "        if peak_memory_lst is not None:\n",
    "            conv_peak_memory = [ModelMemoryUtilities.convert_memory(peak, memory_unit) for peak in peak_memory_lst]\n",
    "        conv_cur_memory = [ModelMemoryUtilities.convert_memory(cur, memory_unit) for cur in cur_memory_lst]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        iterations = range(len(cur_memory_lst))\n",
    "\n",
    "        # Baseline memory line\n",
    "        plt.axhline(y=conv_prev_memory, color='gray', linestyle='--', linewidth=1.5, label=f'Baseline ({conv_prev_memory:.2f} {memory_unit.upper()})')\n",
    "\n",
    "        # Peak and current memory lines\n",
    "        if peak_memory_lst is not None:\n",
    "            plt.plot(iterations, conv_peak_memory, label='Peak Memory', color='#FF5733', linewidth=2.5, linestyle='-', marker='o', markersize=5, alpha=0.85)\n",
    "        plt.plot(iterations, conv_cur_memory, label='Current Memory', color='#3498DB', linewidth=2.5, linestyle='-', marker='x', markersize=5, alpha=0.85)\n",
    "\n",
    "        # Labels and title\n",
    "        plt.xlabel(\"Iterations\", fontsize=12)\n",
    "        plt.ylabel(f\"Memory Usage ({memory_unit.upper()})\", fontsize=12)\n",
    "        plt.title(\"Memory Usage per Iteration\", fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', linewidth=0.6, alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMemoryMonitor:\n",
    "    def __init__(self, model_name, batch_size=1, max_seq_len=4096, torch_dtype=torch.float16, use_amp=False, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize ModelMemoryMonitor for tracking memory usage in inference and training.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Model name or path.\n",
    "            batch_size (int): Number of samples in a batch.\n",
    "            max_seq_len (int): Maximum sequence length.\n",
    "            torch_dtype (torch.dtype): Data type (e.g., torch.float16).\n",
    "            use_amp (bool): If True, enables mixed precision inference.\n",
    "            device (str): Device for inference ('cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.device = device\n",
    "        self.use_amp = use_amp\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=self.torch_dtype).to(device)\n",
    "\n",
    "        # Set pad_token to eos_token if no padding token is available\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def simulate_input_ids(self, sequence_length: int, only_padding=False):\n",
    "        \"\"\"\n",
    "        Generate dummy input IDs for a given sequence length.\n",
    "\n",
    "        Args:\n",
    "            sequence_length (int): Target sequence length.\n",
    "            only_padding (bool): If True, generate only padding tokens.\n",
    "\n",
    "        Returns:\n",
    "            dict: Input IDs and attention masks.\n",
    "        \"\"\"\n",
    "        dummy_text = \"\" if only_padding else \" \".join([\"token\"] * int(sequence_length * 1.5))\n",
    "        inputs = self.tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_seq_len)\n",
    "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "        \n",
    "        actual_length = inputs[\"input_ids\"].shape[1]\n",
    "        if actual_length != sequence_length:\n",
    "            print(f\"Warning: Expected sequence length ({sequence_length}) does not match actual input length ({actual_length}).\")\n",
    "\n",
    "        attention_mask_sum = inputs[\"attention_mask\"].sum().item()\n",
    "        if attention_mask_sum != sequence_length:\n",
    "            print(f\"Warning: Attention mask sum ({attention_mask_sum}) does not match expected sequence length ({sequence_length}).\")\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def test_cuda_forward_memory(self, sample_inputs, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte') -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Measure memory usage during a forward pass.\n",
    "\n",
    "        Args:\n",
    "            sample_inputs (dict): Model input data.\n",
    "            memory_unit (str): Unit for memory display.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Previous, peak, and current memory in specified unit.\n",
    "        \"\"\"\n",
    "        self.model.to(\"cpu\")  # Move model to CPU to measure existing memory reliably\n",
    "        exist_memory = torch.cuda.memory_allocated(self.device)\n",
    "        self.model.to(self.device)  # Move model back to GPU\n",
    "        self.model.eval()\n",
    "\n",
    "        prev_memory = torch.cuda.memory_allocated(self.device)\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**sample_inputs)\n",
    "            output_sum = ModelMemoryUtilities.get_logits(output).sum()\n",
    "        \n",
    "        peak_memory = torch.cuda.max_memory_allocated(self.device) - exist_memory\n",
    "        cur_memory = torch.cuda.memory_allocated(self.device) - exist_memory\n",
    "\n",
    "        prev_memory = ModelMemoryUtilities.convert_memory(prev_memory - exist_memory, memory_unit)\n",
    "        peak_memory = ModelMemoryUtilities.convert_memory(peak_memory, memory_unit)\n",
    "        cur_memory = ModelMemoryUtilities.convert_memory(cur_memory, memory_unit)\n",
    "\n",
    "        print(f\"Previous Memory: {prev_memory:.2f} {memory_unit.upper()}; Peak Memory: {peak_memory:.2f} {memory_unit.upper()}; Current Memory: {cur_memory:.2f} {memory_unit.upper()}\")\n",
    "\n",
    "        return prev_memory, peak_memory, cur_memory\n",
    "    \n",
    "    def test_cuda_iterative_inference_memory(self, prompt: str, max_iters: int = 100, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte') -> Tuple[float, list, list]:\n",
    "        \"\"\"\n",
    "        Estimate memory usage during iterative token-by-token generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Initial prompt for generation.\n",
    "            max_iters (int): Maximum number of iterations (tokens) to generate.\n",
    "            memory_unit (Literal['byte', 'mb', 'gb']): Unit for memory measurement ('byte', 'mb', 'gb').\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, list, list]: Initial memory, list of peak memory per iteration, list of current memory per iteration.\n",
    "        \"\"\"\n",
    "        # Move model to CPU to measure existing memory reliably\n",
    "        self.model.to(\"cpu\")\n",
    "        exist_memory = torch.cuda.memory_allocated(self.device)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Initial memory state before starting the generation loop\n",
    "        prev_memory = torch.cuda.memory_allocated(self.device)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_seq_len)\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        # Lists to track memory usage across iterations\n",
    "        peak_memory_lst, cur_memory_lst = [], []\n",
    "\n",
    "        for i in range(max_iters):\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.use_amp:\n",
    "                    with torch.amp.autocast(device_type=str(self.device)):\n",
    "                        outputs = self.model.generate(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            max_length=input_ids.shape[1] + 1,\n",
    "                            eos_token_id=self.tokenizer.eos_token_id,\n",
    "                            pad_token_id=self.tokenizer.eos_token_id,\n",
    "                            do_sample=False\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=input_ids.shape[1] + 1,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        do_sample=False\n",
    "                    )\n",
    "\n",
    "                next_token_id = outputs[:, -1:]\n",
    "                input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "                # Update attention mask to include the new token\n",
    "                new_attention_mask = torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype, device=self.device)\n",
    "                attention_mask = torch.cat([attention_mask, new_attention_mask], dim=1)\n",
    "\n",
    "                # Stop if EOS token is generated\n",
    "                if next_token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    print(f\"EOS token generated at iteration {i+1}\")\n",
    "                    break\n",
    "\n",
    "            # Record peak and current memory usage\n",
    "            peak_memory_lst.append(torch.cuda.max_memory_allocated(self.device))\n",
    "            cur_memory_lst.append(torch.cuda.memory_allocated(self.device))\n",
    "\n",
    "        # Decode generated text\n",
    "        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        print(f'Generated Text: {generated_text}')\n",
    "\n",
    "        # Adjust for pre-existing memory\n",
    "        prev_memory -= exist_memory\n",
    "        peak_memory_lst = [peak_memory - exist_memory for peak_memory in peak_memory_lst]\n",
    "        cur_memory_lst = [cur_memory - exist_memory for cur_memory in cur_memory_lst]\n",
    "\n",
    "        # Convert memory values to the specified unit\n",
    "        prev_memory = ModelMemoryUtilities.convert_memory(prev_memory, memory_unit)\n",
    "        peak_memory_lst = [ModelMemoryUtilities.convert_memory(mem, memory_unit) for mem in peak_memory_lst]\n",
    "        cur_memory_lst = [ModelMemoryUtilities.convert_memory(mem, memory_unit) for mem in cur_memory_lst]\n",
    "\n",
    "        return prev_memory, peak_memory_lst, cur_memory_lst\n",
    "\n",
    "    def test_cuda_training_memory(self, optimizer_type=torch.optim.Adam, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte') -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"Estimate memory usage in a training step (forward, backward, optimizer).\n",
    "\n",
    "        Args:\n",
    "            optimizer_type (torch.optim): Optimizer to use.\n",
    "            memory_unit (str): Unit of memory measurement.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Highest peak memory and dictionary of memory usage per stage.\n",
    "        \"\"\"\n",
    "        self.model.to(\"cpu\")  # Move model to CPU to measure existing memory reliably\n",
    "        exist_memory = torch.cuda.memory_allocated(self.device)\n",
    "        self.model.to(self.device)  # Move model back to GPU\n",
    "        self.model.train()\n",
    "        pre_memory = torch.cuda.memory_allocated(self.device)\n",
    "\n",
    "        sample_inputs = self.simulate_input_ids(self.max_seq_len)\n",
    "        optimizer = optimizer_type(self.model.parameters(), lr=0.001)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if self.use_amp:\n",
    "            with torch.amp.autocast(device_type=str(self.device)):\n",
    "                output_logits_sum = ModelMemoryUtilities.get_logits(self.model(**sample_inputs)).sum()\n",
    "            output_logits_sum.backward()\n",
    "        else:\n",
    "            output = ModelMemoryUtilities.get_logits(self.model(**sample_inputs)).sum()\n",
    "            output.backward()\n",
    "\n",
    "        # Memory recording for forward, backward, optimizer steps\n",
    "        forward_peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "        backward_peak_memory = torch.cuda.memory_allocated(self.device)\n",
    "        optimizer.step()\n",
    "        optimizer_peak_memory = torch.cuda.memory_allocated(self.device)\n",
    "\n",
    "        # Calculate highest memory usage by stage\n",
    "        peak_memory_dict = {'forward': forward_peak_memory, 'backward': backward_peak_memory, 'optimizer': optimizer_peak_memory}\n",
    "        max_peak_stage, max_peak_memory = max(peak_memory_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "        memory_dict = {\n",
    "            'model_loading': ModelMemoryUtilities.convert_memory(pre_memory - exist_memory, memory_unit),\n",
    "            'forward_pass': ModelMemoryUtilities.convert_memory(forward_peak_memory - exist_memory, memory_unit),\n",
    "            'backward_pass': ModelMemoryUtilities.convert_memory(backward_peak_memory - exist_memory, memory_unit),\n",
    "            'optimize_model': ModelMemoryUtilities.convert_memory(optimizer_peak_memory - exist_memory, memory_unit)\n",
    "        }\n",
    "\n",
    "        print(f\"The training max peak memory: {ModelMemoryUtilities.convert_memory(max_peak_memory, memory_unit)} ({max_peak_stage} stage)\")\n",
    "        print(f\"Training memory consumption: {memory_dict}\")\n",
    "\n",
    "        return max_peak_memory, memory_dict\n",
    "    \n",
    "    def test_cpu_forward_memory(self, sample_inputs, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte') -> Tuple[float, float, float]:\n",
    "        \"\"\"Estimate memory usage on CPU during a forward pass.\"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        prev_memory = ModelMemoryUtilities.measure_cpu_memory()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(**sample_inputs)\n",
    "\n",
    "        cur_memory = ModelMemoryUtilities.measure_cpu_memory() \n",
    "\n",
    "        prev_memory = ModelMemoryUtilities.convert_memory(prev_memory, memory_unit)\n",
    "        cur_memory = ModelMemoryUtilities.convert_memory(cur_memory, memory_unit)\n",
    "\n",
    "        print(f\"Previous Memory: {prev_memory:.2f} {memory_unit.upper()}; Current Memory: {cur_memory:.2f} {memory_unit.upper()}\")\n",
    "        \n",
    "        return prev_memory, cur_memory\n",
    "    \n",
    "    def test_cpu_iterative_inference_memory(self, prompt: str, max_iters: int = 100, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte') -> Tuple[float, list, list]:\n",
    "        \"\"\"Estimate memory usage on CPU during iterative token-by-token generation.\"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        prev_memory = ModelMemoryUtilities.measure_cpu_memory()\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_seq_len)\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        cur_memory_lst = []\n",
    "\n",
    "        for i in range(max_iters):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=input_ids.shape[1] + 1,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                next_token_id = outputs[:, -1:]\n",
    "                input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "                # Update attention mask to include the new token\n",
    "                new_attention_mask = torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype, device=self.device)\n",
    "                attention_mask = torch.cat([attention_mask, new_attention_mask], dim=1)\n",
    "\n",
    "                if next_token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    print(f\"EOS token generated at iteration {i+1}\")\n",
    "                    break\n",
    "\n",
    "            \n",
    "            cur_memory_lst.append(ModelMemoryUtilities.measure_cpu_memory())\n",
    "\n",
    "        # Decode generated text\n",
    "        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        print(f'Generated Text: {generated_text}')\n",
    "\n",
    "        cur_memory_lst = [ModelMemoryUtilities.convert_memory(mem, memory_unit) for mem in cur_memory_lst]\n",
    "\n",
    "        return ModelMemoryUtilities.convert_memory(prev_memory, memory_unit),  cur_memory_lst\n",
    "    \n",
    "    def test_cpu_training_memory(self, optimizer_type=torch.optim.Adam, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte') -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"Estimate memory usage on CPU during a training step (forward, backward, optimizer).\"\"\"\n",
    "        self.model.train()\n",
    "        prev_memory = ModelMemoryUtilities.measure_cpu_memory()\n",
    "\n",
    "        sample_inputs = self.simulate_input_ids(self.max_seq_len)\n",
    "        optimizer = optimizer_type(self.model.parameters(), lr=0.001)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        forward_memory, backward_memory, optimizer_memory = 0, 0, 0\n",
    "\n",
    "        output_logits_sum = ModelMemoryUtilities.get_logits(self.model(**sample_inputs)).sum()\n",
    "        forward_memory = ModelMemoryUtilities.measure_cpu_memory()\n",
    "\n",
    "        output_logits_sum.backward()\n",
    "        backward_memory = ModelMemoryUtilities.measure_cpu_memory()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer_memory = ModelMemoryUtilities.measure_cpu_memory()\n",
    "\n",
    "\n",
    "\n",
    "        memory_dict = {\n",
    "            'model_loading': ModelMemoryUtilities.convert_memory(prev_memory, memory_unit),\n",
    "            'forward_pass': ModelMemoryUtilities.convert_memory(forward_memory, memory_unit),\n",
    "            'backward_pass': ModelMemoryUtilities.convert_memory(backward_memory, memory_unit),\n",
    "            'optimize_model': ModelMemoryUtilities.convert_memory(optimizer_memory, memory_unit)\n",
    "        }\n",
    "\n",
    "        print(f\"Training memory consumption: {memory_dict}\")\n",
    "\n",
    "        return  memory_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ModelMemoryMonitor:\n",
    "#     def __init__(self, model_name, batch_size=1, max_seq_len=4096, torch_dtype=torch.float16, use_amp=False, device=\"cuda\"):\n",
    "#         \"\"\"\n",
    "#         Initialize the ModelMemoryMonitor for tracking model inference memory usage.\n",
    "\n",
    "#         Args:\n",
    "#             model_name (str): Hugging Face model name or local path.\n",
    "#             batch_size (int): Number of samples in a batch.\n",
    "#             max_seq_len (int): Maximum sequence length.\n",
    "#             torch_dtype (torch.dtype): Data type (e.g., torch.float16).\n",
    "#             use_amp (bool): If True, enables mixed precision inference.\n",
    "#             device (str): Device for inference ('cuda' or 'cpu').\n",
    "#         \"\"\"\n",
    "#         self.model_name = model_name\n",
    "#         self.torch_dtype = torch_dtype\n",
    "#         self.device = device\n",
    "#         self.use_amp = use_amp\n",
    "#         self.batch_size = batch_size\n",
    "#         self.max_seq_len = max_seq_len\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=self.torch_dtype).to(device)\n",
    "\n",
    "#         # Set pad_token to eos_token if no padding token is available\n",
    "#         if self.tokenizer.pad_token is None:\n",
    "#             self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "#     def reinitialize_model(self):\n",
    "#         \"\"\"Reinitialize the model (useful for testing or resetting).\"\"\"\n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=self.torch_dtype).to(self.device)\n",
    "\n",
    "#     def delete_model(self):\n",
    "#         \"\"\"Delete the model and clear the GPU cache.\"\"\"\n",
    "#         del self.model\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         print(\"Model deleted and CUDA cache cleared.\")\n",
    "\n",
    "#     def convert_memory(self, memory, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "#         \"\"\"Convert memory to specified unit.\"\"\"\n",
    "#         if memory_unit == 'mb':\n",
    "#             return memory / 1048576  # Convert bytes to MB\n",
    "#         elif memory_unit == 'gb':\n",
    "#             return memory / 1073741824  # Convert bytes to GB\n",
    "#         return memory  # Default to bytes\n",
    "    \n",
    "#     def get_logits(self, model_output):\n",
    "#         \"\"\"\n",
    "#         Extracts logits from the model output. Supports different output types.\n",
    "\n",
    "#             Args:\n",
    "#             model_output: Output from the model's forward pass.\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: The logits tensor.\n",
    "\n",
    "#         Raises:\n",
    "#             ValueError: If logits cannot be found in the output.\n",
    "#         \"\"\"\n",
    "#         # Check for `logits` attribute (used by most Hugging Face models)\n",
    "#         if hasattr(model_output, 'logits'):\n",
    "#             return model_output.logits\n",
    "#         # If the output is directly a tensor, assume it's the logits\n",
    "#         elif isinstance(model_output, torch.Tensor):\n",
    "#             return model_output\n",
    "#         else:\n",
    "#             raise ValueError(\"Model output does not contain logits or is not a tensor.\")\n",
    "\n",
    "\n",
    "#     def simulate_input_ids(self, sequence_length: int, only_padding=False):\n",
    "#         \"\"\"\n",
    "#         Generate dummy input IDs of specified sequence length.\n",
    "\n",
    "#         Args:\n",
    "#             sequence_length (int): Desired sequence length for input IDs.\n",
    "#             only_padding (bool): If True, generates only padding tokens.\n",
    "\n",
    "#         Returns:\n",
    "#             dict: A dictionary with input IDs and attention masks on the specified device.\n",
    "#         \"\"\"\n",
    "#         dummy_text = \"\" if only_padding else \" \".join([\"token\"] * int(sequence_length * 1.5))\n",
    "#         inputs = self.tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_seq_len)\n",
    "#         inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "\n",
    "#         # Validate sequence length and attention mask sum\n",
    "#         actual_length = inputs[\"input_ids\"].shape[1]\n",
    "#         if actual_length != sequence_length:\n",
    "#             print(f\"Warning: Expected sequence length ({sequence_length}) does not match actual input length ({actual_length}).\")\n",
    "\n",
    "#         attention_mask_sum = inputs[\"attention_mask\"].sum().item()\n",
    "#         if attention_mask_sum != sequence_length:\n",
    "#             print(f\"Warning: Attention mask sum ({attention_mask_sum}) does not match expected sequence length ({sequence_length}).\")\n",
    "\n",
    "#         return inputs\n",
    "\n",
    "#     def test_forward_memory(self, sample_inputs, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "#         \"\"\"\n",
    "#         Estimate memory usage during a forward pass.\n",
    "\n",
    "#         Args:\n",
    "#             sample_inputs (dict): Input data for the model.\n",
    "#             memory_unit (Literal['byte', 'mb', 'gb']): Unit of memory measurement ('byte', 'mb', 'gb').\n",
    "\n",
    "#         Returns:\n",
    "#             tuple: Previous, peak, and current memory in the specified unit.\n",
    "#         \"\"\"\n",
    "#         self.model.cpu()\n",
    "#         exist_memory = torch.cuda.memory_allocated(self.device)\n",
    "#         self.model.to(self.device)\n",
    "#         self.model.eval()\n",
    "\n",
    "#         # Track initial memory usage\n",
    "#         prev_memory = torch.cuda.memory_allocated(self.device)\n",
    "#         torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#         # Perform inference with optional AMP (mixed precision)\n",
    "#         with torch.no_grad():\n",
    "#             if self.use_amp:\n",
    "#                 with torch.amp.autocast(device_type=str(self.device)):\n",
    "#                     outputs = self.model(**sample_inputs)\n",
    "#             else:\n",
    "#                 outputs = self.model(**sample_inputs)\n",
    "\n",
    "#         # Get peak and current memory usage\n",
    "#         peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "#         cur_memory = torch.cuda.memory_allocated(self.device)\n",
    "\n",
    "#         # delete pre-existing memory\n",
    "#         prev_memory -= exist_memory\n",
    "#         peak_memory -= exist_memory\n",
    "#         cur_memory -= exist_memory\n",
    "\n",
    "#         # Convert memory measurements to the specified unit\n",
    "#         prev_memory = self.convert_memory(prev_memory, memory_unit)\n",
    "#         peak_memory = self.convert_memory(peak_memory, memory_unit)\n",
    "#         cur_memory = self.convert_memory(cur_memory, memory_unit)\n",
    "\n",
    "#         # Display memory usage summary\n",
    "#         print(f\"Previous Memory: {prev_memory:.2f} {memory_unit.upper()}; Peak Memory: {peak_memory:.2f} {memory_unit.upper()}; Current Memory: {cur_memory:.2f} {memory_unit.upper()}\")\n",
    "#         print(f\"Peak Memory Difference: {peak_memory - prev_memory:.2f} {memory_unit.upper()}\")\n",
    "#         print(f\"Total Memory Consumption: {cur_memory - prev_memory:.2f} {memory_unit.upper()}\")\n",
    "\n",
    "#         return prev_memory, peak_memory, cur_memory\n",
    "\n",
    "#     def test_iterative_inference_memory(self, prompt, max_iters=100):\n",
    "#         \"\"\"\n",
    "#         Estimate memory usage during iterative token-by-token generation.\n",
    "\n",
    "#         Args:\n",
    "#             prompt (str): Initial prompt for generation.\n",
    "#             max_iters (int): Maximum number of iterations (tokens) to generate.\n",
    "\n",
    "#         Returns:\n",
    "#             tuple: Initial memory, list of peak memory per iteration, list of current memory per iteration.\n",
    "#         \"\"\"\n",
    "#         self.model.cpu()\n",
    "#         exist_memory = torch.cuda.memory_allocated(self.device)\n",
    "#         self.model.to(self.device)\n",
    "#         self.model.eval()\n",
    "#         prev_memory = torch.cuda.memory_allocated(self.device)\n",
    "#         inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_seq_len)\n",
    "#         input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "#         attention_mask = inputs[\"attention_mask\"].to(self.device) \n",
    "\n",
    "#         peak_memory_lst, cur_memory_lst = [], []\n",
    "\n",
    "#         for i in range(max_iters):\n",
    "#             torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 if self.use_amp:\n",
    "#                     with torch.amp.autocast(device_type=str(self.device)):\n",
    "#                         outputs = self.model.generate(\n",
    "#                             input_ids,\n",
    "#                             attention_mask=attention_mask,\n",
    "#                             max_length=input_ids.shape[1] + 1,\n",
    "#                             eos_token_id=self.tokenizer.eos_token_id,\n",
    "#                             pad_token_id=self.tokenizer.eos_token_id,\n",
    "#                             do_sample=False\n",
    "#                         )\n",
    "#                 else:\n",
    "#                     outputs = self.model.generate(\n",
    "#                         input_ids,\n",
    "#                         attention_mask=attention_mask,\n",
    "#                         max_length=input_ids.shape[1] + 1,\n",
    "#                         eos_token_id=self.tokenizer.eos_token_id,\n",
    "#                         pad_token_id=self.tokenizer.eos_token_id,\n",
    "#                         do_sample=False\n",
    "#                     )\n",
    "\n",
    "#                 next_token_id = outputs[:, -1:]\n",
    "#                 input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "#                 # Update attention mask to include the new token\n",
    "#                 new_attention_mask = torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype, device=self.device)\n",
    "#                 attention_mask = torch.cat([attention_mask, new_attention_mask], dim=1)\n",
    "\n",
    "#                 if next_token_id.item() == self.tokenizer.eos_token_id:\n",
    "#                     print(f\"EOS token generated at iteration {i+1}\")\n",
    "#                     break\n",
    "\n",
    "#             peak_memory_lst.append(torch.cuda.max_memory_allocated(self.device))\n",
    "#             cur_memory_lst.append(torch.cuda.memory_allocated(self.device))\n",
    "\n",
    "#         generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "#         print(f'Generated Text: {generated_text}')\n",
    "\n",
    "#         # - delete pre-existing memory\n",
    "#         prev_memory -= exist_memory\n",
    "#         peak_memory_lst = [peak_memory - exist_memory for peak_memory in peak_memory_lst]\n",
    "#         cur_memory_lst = [cur_memory - exist_memory for cur_memory in cur_memory_lst]\n",
    "\n",
    "#         return prev_memory, peak_memory_lst, cur_memory_lst\n",
    "    \n",
    "#     def test_training_memory(self, optimizer_type=torch.optim.Adam, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte' ):\n",
    "#         device = self.device\n",
    "#         self.model.cpu()\n",
    "#         torch.cuda.reset_peak_memory_stats()\n",
    "#         exist_memory = torch.cuda.memory_allocated(device)\n",
    "#         self.model.to(device)\n",
    "#         self.model.train()\n",
    "#         pre_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "#         sample_inputs = self.simulate_input_ids(self.max_seq_len)\n",
    "#         optimizer = optimizer_type(self.model.parameters(), lr=.001)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         if use_amp:\n",
    "#             with torch.amp.autocast(device_type=str(self.device)):\n",
    "#                 output = self.model(**sample_inputs)\n",
    "#                 output_logits = self.get_logits(output)\n",
    "#                 output_logits_sum = output_logits.sum()\n",
    "#                 forward_memory = torch.cuda.memory_allocated(device)\n",
    "#                 forward_peak_memory = torch.cuda.max_memory_allocated(device)\n",
    "#                 torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#             output_logits_sum.backward()\n",
    "#             backward_memory = torch.cuda.memory_allocated(device)\n",
    "#             backward_peak_memory = torch.cuda.max_memory_allocated(device)\n",
    "#             torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#             optimizer.step()\n",
    "#             optimizer_memory = torch.cuda.memory_allocated(device)\n",
    "#             optimizer_peak_memory = torch.cuda.max_memory_allocated(device)\n",
    "\n",
    "\n",
    "#         else:\n",
    "#             # Forward pass without AMP\n",
    "#             output = self.model(**sample_inputs).sum()\n",
    "#             forward_memory = torch.cuda.memory_allocated(self.device)\n",
    "#             forward_peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "#             torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#             # Backward pass without AMP\n",
    "#             output.backward()\n",
    "#             backward_memory = torch.cuda.memory_allocated(self.device)\n",
    "#             backward_peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "#             torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#             # Optimizer step without AMP\n",
    "#             optimizer.step()\n",
    "#             optimizer_memory = torch.cuda.memory_allocated(self.device)\n",
    "#             optimizer_peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "\n",
    "#         # - delete existing memory\n",
    "#         pre_memory -= exist_memory\n",
    "#         forward_memory -= exist_memory\n",
    "#         forward_peak_memory -= exist_memory\n",
    "#         backward_memory -= exist_memory\n",
    "#         backward_peak_memory -= exist_memory\n",
    "#         optimizer_memory -= exist_memory\n",
    "#         optimizer_peak_memory -= exist_memory\n",
    "\n",
    "#         peak_memory_dict = {\n",
    "#             'forward': forward_peak_memory,\n",
    "#             'backward': backward_peak_memory,\n",
    "#             'optimizer': optimizer_peak_memory\n",
    "#         }\n",
    "\n",
    "        \n",
    "#         max_peak_stage, max_peak_memory = max(peak_memory_dict.items(), key=lambda x: x[1])\n",
    "        \n",
    "#         memory_dict = {'model_loading': pre_memory, 'forward_pass': forward_memory, 'backward_pass': backward_memory, 'optimize_model': optimizer_memory }\n",
    "\n",
    "#         print(f\"The training max peak memory: {self.convert_memory(max_peak_memory, memory_unit)} ({max_peak_stage} stage)\")\n",
    "#         converted_memory_dict = {key: self.convert_memory(value, memory_unit) for key, value in memory_dict.items()}\n",
    "#         print(f\"the training memory consumption: {converted_memory_dict}\")\n",
    "\n",
    "#         return max_peak_memory, memory_dict\n",
    "\n",
    "\n",
    "#     def draw_memory_lines(self, prev_memory, peak_memory_lst, cur_memory_lst, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "#         \"\"\"\n",
    "#         Plot memory usage over iterations.\n",
    "\n",
    "#         Args:\n",
    "#             prev_memory (int): Baseline memory usage.\n",
    "#             peak_memory_lst (list[int]): List of peak memory usage per iteration.\n",
    "#             cur_memory_lst (list[int]): List of current memory usage per iteration.\n",
    "#             memory_unit (Literal['byte', 'mb', 'gb']): Unit for memory measurements ('byte', 'mb', 'gb').\n",
    "#         \"\"\"\n",
    "#         conv_prev_memory = self.convert_memory(prev_memory, memory_unit)\n",
    "#         conv_peak_memory = [self.convert_memory(peak, memory_unit) for peak in peak_memory_lst]\n",
    "#         conv_cur_memory = [self.convert_memory(cur, memory_unit) for cur in cur_memory_lst]\n",
    "\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         iterations = range(len(peak_memory_lst))\n",
    "\n",
    "#         # Baseline memory line\n",
    "#         plt.axhline(y=conv_prev_memory, color='gray', linestyle='--', linewidth=1.5, label=f'Baseline ({conv_prev_memory:.2f} {memory_unit.upper()})')\n",
    "\n",
    "#         # Peak and current memory lines\n",
    "#         plt.plot(iterations, conv_peak_memory, label='Peak Memory', color='#FF5733', linewidth=2.5, linestyle='-', marker='o', markersize=5, alpha=0.85)\n",
    "#         plt.plot(iterations, conv_cur_memory, label='Current Memory', color='#3498DB', linewidth=2.5, linestyle='-', marker='x', markersize=5, alpha=0.85)\n",
    "\n",
    "#         # Labels and title\n",
    "#         plt.xlabel(\"Iterations\", fontsize=12)\n",
    "#         plt.ylabel(f\"Memory Usage ({memory_unit.upper()})\", fontsize=12)\n",
    "#         plt.title(\"Memory Usage per Iteration\", fontsize=16, fontweight='bold')\n",
    "#         plt.legend()\n",
    "#         plt.grid(True, linestyle='--', linewidth=0.6, alpha=0.7)\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Test estimate forward memory method\n",
    "# monitor = ModelMemoryMonitor(llama7b_model_name, use_amp=use_amp, device=device)\n",
    "# sample_inputs = monitor.simulate_input_ids(1)\n",
    "# monitor.test_cuda_forward_memory(sample_inputs, memory_unit='mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test estimate inference memory method\n",
    "# prompt = (\n",
    "#     'You should generate 100 words to explain diffusion model: '\n",
    "# )\n",
    "# monitor = ModelMemoryMonitor(llama7b_model_name, use_amp=use_amp, device=device)\n",
    "# prev_memory, peak_memory_lst, cur_memory_lst = monitor.test_cuda_iterative_inference_memory(prompt)\n",
    "# ModelMemoryUtilities.draw_memory_lines(prev_memory, cur_memory_lst, peak_memory_lst, memory_unit='gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test test training memory method\n",
    "# monitor = ModelMemoryMonitor(tinyllama_model_name, use_amp=use_amp, device=device)\n",
    "# peak_memory, memory_dict = monitor.test_cuda_training_memory(memory_unit='gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test cpu test forward memory method\n",
    "# monitor = ModelMemoryMonitor(tinyllama_model_name, use_amp=use_amp, device=device)\n",
    "# sample_inputs = monitor.simulate_input_ids(1)\n",
    "# monitor.test_cpu_forward_memory(sample_inputs, memory_unit='mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d985990a904084b46b2c0a80e48bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuz165/LLM-Simulator/llm-simulator-env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yuz165/LLM-Simulator/llm-simulator-env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: You should generate 100 words to explain diffusion model: 1. The diffusion model is a model of how information spreads through a population. 2. The diffusion model is a model of how information spreads through a population. 3. The diffusion model is a model of how information spreads through a population. 4. The diffusion model is a model of how information spreads through a population. 5. The diffusion model is a model of how information spreads through a population. 6. The diffusion model is a model of how\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Test cpu inference memory method\n",
    "# prompt = (\n",
    "#     'You should generate 100 words to explain diffusion model: '\n",
    "# )\n",
    "# monitor = ModelMemoryMonitor(llama7b_model_name, use_amp=use_amp, device=device)\n",
    "# prev_memory, cur_memory_lst = monitor.test_cpu_iterative_inference_memory(prompt)\n",
    "\n",
    "# ModelMemoryUtilities.draw_memory_lines(prev_memory, cur_memory_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training memory consumption: {'model_loading': 3.194774627685547, 'forward_pass': 3.1950225830078125, 'backward_pass': 3.1960182189941406, 'optimize_model': 3.1962547302246094}\n"
     ]
    }
   ],
   "source": [
    "# Test test training memory method\n",
    "monitor = ModelMemoryMonitor(tinyllama_model_name, use_amp=use_amp, device=device)\n",
    "memory_dict = monitor.test_cpu_training_memory(memory_unit='gb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-simulator-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
