{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Literal\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Available Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load Models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current Available Device: {device}\")\n",
    "use_amp = amp_supported = torch.cuda.is_available() and torch.cuda.get_device_capability(0) >= (7, 0)\n",
    "tinyllama_model_name = \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "llama7b_model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "# local_model_dir = './model_cache'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n",
    "# Load model directly\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\",   torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Memory and Max Memory Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(\"Tinyllama/Tinyllama-1.1B-step-50K-105b\")\n",
    "\n",
    "# hidden_size = config.hidden_size\n",
    "# eos_token_id = config.eos_token_id\n",
    "# max_position_embeddings = config.max_position_embeddings\n",
    "\n",
    "# print(\"Hidden Size:\", hidden_size)\n",
    "# print(\"EOS Token ID:\", eos_token_id)\n",
    "# print(\"Max Position Embeddings:\", max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "b = 1 # batch size\n",
    "s = 1 # sequence length\n",
    "max_s = 4096 # max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelMemoryUtilities:\n",
    "    @staticmethod\n",
    "    def convert_memory(memory: int, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte') -> float:\n",
    "        \"\"\"Convert memory to the specified unit.\"\"\"\n",
    "        if memory_unit == 'mb':\n",
    "            return memory / 1048576  # Convert bytes to MB\n",
    "        elif memory_unit == 'gb':\n",
    "            return memory / 1073741824  # Convert bytes to GB\n",
    "        return memory  # Default to bytes\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logits(model_output) -> torch.Tensor:\n",
    "        \"\"\"Extract logits from model output, supporting various output formats.\"\"\"\n",
    "        if hasattr(model_output, 'logits'):\n",
    "            return model_output.logits\n",
    "        elif isinstance(model_output, torch.Tensor):\n",
    "            return model_output\n",
    "        else:\n",
    "            raise ValueError(\"Model output does not contain logits or is not a tensor.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_memory_lines(prev_memory: int, peak_memory_lst: list, cur_memory_lst: list, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "        \"\"\"Plot memory usage over iterations.\"\"\"\n",
    "        conv_prev_memory = ModelMemoryUtilities.convert_memory(prev_memory, memory_unit)\n",
    "        conv_peak_memory = [ModelMemoryUtilities.convert_memory(peak, memory_unit) for peak in peak_memory_lst]\n",
    "        conv_cur_memory = [ModelMemoryUtilities.convert_memory(cur, memory_unit) for cur in cur_memory_lst]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        iterations = range(len(peak_memory_lst))\n",
    "\n",
    "        # Baseline memory line\n",
    "        plt.axhline(y=conv_prev_memory, color='gray', linestyle='--', linewidth=1.5, label=f'Baseline ({conv_prev_memory:.2f} {memory_unit.upper()})')\n",
    "\n",
    "        # Peak and current memory lines\n",
    "        plt.plot(iterations, conv_peak_memory, label='Peak Memory', color='#FF5733', linewidth=2.5, linestyle='-', marker='o', markersize=5, alpha=0.85)\n",
    "        plt.plot(iterations, conv_cur_memory, label='Current Memory', color='#3498DB', linewidth=2.5, linestyle='-', marker='x', markersize=5, alpha=0.85)\n",
    "\n",
    "        # Labels and title\n",
    "        plt.xlabel(\"Iterations\", fontsize=12)\n",
    "        plt.ylabel(f\"Memory Usage ({memory_unit.upper()})\", fontsize=12)\n",
    "        plt.title(\"Memory Usage per Iteration\", fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', linewidth=0.6, alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelMemoryMonitor:\n",
    "    def __init__(self, model_name, batch_size=1, max_seq_len=4096, torch_dtype=torch.float16, use_amp=False, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize the ModelMemoryMonitor for tracking model inference memory usage.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Hugging Face model name or local path.\n",
    "            batch_size (int): Number of samples in a batch.\n",
    "            max_seq_len (int): Maximum sequence length.\n",
    "            torch_dtype (torch.dtype): Data type (e.g., torch.float16).\n",
    "            use_amp (bool): If True, enables mixed precision inference.\n",
    "            device (str): Device for inference ('cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.device = device\n",
    "        self.use_amp = use_amp\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=self.torch_dtype).to(device)\n",
    "\n",
    "        # Set pad_token to eos_token if no padding token is available\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def reinitialize_model(self):\n",
    "        \"\"\"Reinitialize the model (useful for testing or resetting).\"\"\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=self.torch_dtype).to(self.device)\n",
    "\n",
    "    def delete_model(self):\n",
    "        \"\"\"Delete the model and clear the GPU cache.\"\"\"\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Model deleted and CUDA cache cleared.\")\n",
    "\n",
    "    def convert_memory(self, memory, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "        \"\"\"Convert memory to specified unit.\"\"\"\n",
    "        if memory_unit == 'mb':\n",
    "            return memory / 1048576  # Convert bytes to MB\n",
    "        elif memory_unit == 'gb':\n",
    "            return memory / 1073741824  # Convert bytes to GB\n",
    "        return memory  # Default to bytes\n",
    "    \n",
    "    def get_logits(self, model_output):\n",
    "        \"\"\"\n",
    "        Extracts logits from the model output. Supports different output types.\n",
    "\n",
    "            Args:\n",
    "            model_output: Output from the model's forward pass.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The logits tensor.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If logits cannot be found in the output.\n",
    "        \"\"\"\n",
    "        # Check for `logits` attribute (used by most Hugging Face models)\n",
    "        if hasattr(model_output, 'logits'):\n",
    "            return model_output.logits\n",
    "        # If the output is directly a tensor, assume it's the logits\n",
    "        elif isinstance(model_output, torch.Tensor):\n",
    "            return model_output\n",
    "        else:\n",
    "            raise ValueError(\"Model output does not contain logits or is not a tensor.\")\n",
    "\n",
    "\n",
    "    def simulate_input_ids(self, sequence_length: int, only_padding=False):\n",
    "        \"\"\"\n",
    "        Generate dummy input IDs of specified sequence length.\n",
    "\n",
    "        Args:\n",
    "            sequence_length (int): Desired sequence length for input IDs.\n",
    "            only_padding (bool): If True, generates only padding tokens.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with input IDs and attention masks on the specified device.\n",
    "        \"\"\"\n",
    "        dummy_text = \"\" if only_padding else \" \".join([\"token\"] * int(sequence_length * 1.5))\n",
    "        inputs = self.tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_seq_len)\n",
    "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "\n",
    "        # Validate sequence length and attention mask sum\n",
    "        actual_length = inputs[\"input_ids\"].shape[1]\n",
    "        if actual_length != sequence_length:\n",
    "            print(f\"Warning: Expected sequence length ({sequence_length}) does not match actual input length ({actual_length}).\")\n",
    "\n",
    "        attention_mask_sum = inputs[\"attention_mask\"].sum().item()\n",
    "        if attention_mask_sum != sequence_length:\n",
    "            print(f\"Warning: Attention mask sum ({attention_mask_sum}) does not match expected sequence length ({sequence_length}).\")\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def test_forward_memory(self, sample_inputs, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "        \"\"\"\n",
    "        Estimate memory usage during a forward pass.\n",
    "\n",
    "        Args:\n",
    "            sample_inputs (dict): Input data for the model.\n",
    "            memory_unit (Literal['byte', 'mb', 'gb']): Unit of memory measurement ('byte', 'mb', 'gb').\n",
    "\n",
    "        Returns:\n",
    "            tuple: Previous, peak, and current memory in the specified unit.\n",
    "        \"\"\"\n",
    "        self.model.cpu()\n",
    "        exist_memory = torch.cuda.memory_allocated(self.device)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Track initial memory usage\n",
    "        prev_memory = torch.cuda.memory_allocated(self.device)\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # Perform inference with optional AMP (mixed precision)\n",
    "        with torch.no_grad():\n",
    "            if self.use_amp:\n",
    "                with torch.amp.autocast(device_type=str(self.device)):\n",
    "                    outputs = self.model(**sample_inputs)\n",
    "            else:\n",
    "                outputs = self.model(**sample_inputs)\n",
    "\n",
    "        # Get peak and current memory usage\n",
    "        peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "        cur_memory = torch.cuda.memory_allocated(self.device)\n",
    "\n",
    "        # delete pre-existing memory\n",
    "        prev_memory -= exist_memory\n",
    "        peak_memory -= exist_memory\n",
    "        cur_memory -= exist_memory\n",
    "\n",
    "        # Convert memory measurements to the specified unit\n",
    "        prev_memory = self.convert_memory(prev_memory, memory_unit)\n",
    "        peak_memory = self.convert_memory(peak_memory, memory_unit)\n",
    "        cur_memory = self.convert_memory(cur_memory, memory_unit)\n",
    "\n",
    "        # Display memory usage summary\n",
    "        print(f\"Previous Memory: {prev_memory:.2f} {memory_unit.upper()}; Peak Memory: {peak_memory:.2f} {memory_unit.upper()}; Current Memory: {cur_memory:.2f} {memory_unit.upper()}\")\n",
    "        print(f\"Peak Memory Difference: {peak_memory - prev_memory:.2f} {memory_unit.upper()}\")\n",
    "        print(f\"Total Memory Consumption: {cur_memory - prev_memory:.2f} {memory_unit.upper()}\")\n",
    "\n",
    "        return prev_memory, peak_memory, cur_memory\n",
    "\n",
    "    def test_iterative_inference_memory(self, prompt, max_iters=100):\n",
    "        \"\"\"\n",
    "        Estimate memory usage during iterative token-by-token generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Initial prompt for generation.\n",
    "            max_iters (int): Maximum number of iterations (tokens) to generate.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Initial memory, list of peak memory per iteration, list of current memory per iteration.\n",
    "        \"\"\"\n",
    "        self.model.cpu()\n",
    "        exist_memory = torch.cuda.memory_allocated(self.device)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        prev_memory = torch.cuda.memory_allocated(self.device)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_seq_len)\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.device) \n",
    "\n",
    "        peak_memory_lst, cur_memory_lst = [], []\n",
    "\n",
    "        for i in range(max_iters):\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.use_amp:\n",
    "                    with torch.amp.autocast(device_type=str(self.device)):\n",
    "                        outputs = self.model.generate(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            max_length=input_ids.shape[1] + 1,\n",
    "                            eos_token_id=self.tokenizer.eos_token_id,\n",
    "                            pad_token_id=self.tokenizer.eos_token_id,\n",
    "                            do_sample=False\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=input_ids.shape[1] + 1,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        do_sample=False\n",
    "                    )\n",
    "\n",
    "                next_token_id = outputs[:, -1:]\n",
    "                input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "                # Update attention mask to include the new token\n",
    "                new_attention_mask = torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype, device=self.device)\n",
    "                attention_mask = torch.cat([attention_mask, new_attention_mask], dim=1)\n",
    "\n",
    "                if next_token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    print(f\"EOS token generated at iteration {i+1}\")\n",
    "                    break\n",
    "\n",
    "            peak_memory_lst.append(torch.cuda.max_memory_allocated(self.device))\n",
    "            cur_memory_lst.append(torch.cuda.memory_allocated(self.device))\n",
    "\n",
    "        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        print(f'Generated Text: {generated_text}')\n",
    "\n",
    "        # - delete pre-existing memory\n",
    "        prev_memory -= exist_memory\n",
    "        peak_memory_lst = [peak_memory - exist_memory for peak_memory in peak_memory_lst]\n",
    "        cur_memory_lst = [cur_memory - exist_memory for cur_memory in cur_memory_lst]\n",
    "\n",
    "        return prev_memory, peak_memory_lst, cur_memory_lst\n",
    "    \n",
    "    def test_training_memory(self, optimizer_type=torch.optim.Adam, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte' ):\n",
    "        device = self.device\n",
    "        self.model.cpu()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        exist_memory = torch.cuda.memory_allocated(device)\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        pre_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "        sample_inputs = self.simulate_input_ids(self.max_seq_len)\n",
    "        optimizer = optimizer_type(self.model.parameters(), lr=.001)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.amp.autocast(device_type=str(self.device)):\n",
    "                output = self.model(**sample_inputs)\n",
    "                output_logits = self.get_logits(output)\n",
    "                output_logits_sum = output_logits.sum()\n",
    "                forward_memory = torch.cuda.memory_allocated(device)\n",
    "                forward_peak_memory = torch.cuda.max_memory_allocated(device)\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            output_logits_sum.backward()\n",
    "            backward_memory = torch.cuda.memory_allocated(device)\n",
    "            backward_peak_memory = torch.cuda.max_memory_allocated(device)\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer_memory = torch.cuda.memory_allocated(device)\n",
    "            optimizer_peak_memory = torch.cuda.max_memory_allocated(device)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Forward pass without AMP\n",
    "            output = self.model(**sample_inputs).sum()\n",
    "            forward_memory = torch.cuda.memory_allocated(self.device)\n",
    "            forward_peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            # Backward pass without AMP\n",
    "            output.backward()\n",
    "            backward_memory = torch.cuda.memory_allocated(self.device)\n",
    "            backward_peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            # Optimizer step without AMP\n",
    "            optimizer.step()\n",
    "            optimizer_memory = torch.cuda.memory_allocated(self.device)\n",
    "            optimizer_peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "\n",
    "        # - delete existing memory\n",
    "        pre_memory -= exist_memory\n",
    "        forward_memory -= exist_memory\n",
    "        forward_peak_memory -= exist_memory\n",
    "        backward_memory -= exist_memory\n",
    "        backward_peak_memory -= exist_memory\n",
    "        optimizer_memory -= exist_memory\n",
    "        optimizer_peak_memory -= exist_memory\n",
    "\n",
    "        peak_memory_dict = {\n",
    "            'forward': forward_peak_memory,\n",
    "            'backward': backward_peak_memory,\n",
    "            'optimizer': optimizer_peak_memory\n",
    "        }\n",
    "\n",
    "        \n",
    "        max_peak_stage, max_peak_memory = max(peak_memory_dict.items(), key=lambda x: x[1])\n",
    "        \n",
    "        memory_dict = {'model_loading': pre_memory, 'forward_pass': forward_memory, 'backward_pass': backward_memory, 'optimize_model': optimizer_memory }\n",
    "\n",
    "        print(f\"The training max peak memory: {self.convert_memory(max_peak_memory, memory_unit)} ({max_peak_stage} stage)\")\n",
    "        converted_memory_dict = {key: self.convert_memory(value, memory_unit) for key, value in memory_dict.items()}\n",
    "        print(f\"the training memory consumption: {converted_memory_dict}\")\n",
    "\n",
    "        return max_peak_memory, memory_dict\n",
    "\n",
    "\n",
    "    def draw_memory_lines(self, prev_memory, peak_memory_lst, cur_memory_lst, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "        \"\"\"\n",
    "        Plot memory usage over iterations.\n",
    "\n",
    "        Args:\n",
    "            prev_memory (int): Baseline memory usage.\n",
    "            peak_memory_lst (list[int]): List of peak memory usage per iteration.\n",
    "            cur_memory_lst (list[int]): List of current memory usage per iteration.\n",
    "            memory_unit (Literal['byte', 'mb', 'gb']): Unit for memory measurements ('byte', 'mb', 'gb').\n",
    "        \"\"\"\n",
    "        conv_prev_memory = self.convert_memory(prev_memory, memory_unit)\n",
    "        conv_peak_memory = [self.convert_memory(peak, memory_unit) for peak in peak_memory_lst]\n",
    "        conv_cur_memory = [self.convert_memory(cur, memory_unit) for cur in cur_memory_lst]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        iterations = range(len(peak_memory_lst))\n",
    "\n",
    "        # Baseline memory line\n",
    "        plt.axhline(y=conv_prev_memory, color='gray', linestyle='--', linewidth=1.5, label=f'Baseline ({conv_prev_memory:.2f} {memory_unit.upper()})')\n",
    "\n",
    "        # Peak and current memory lines\n",
    "        plt.plot(iterations, conv_peak_memory, label='Peak Memory', color='#FF5733', linewidth=2.5, linestyle='-', marker='o', markersize=5, alpha=0.85)\n",
    "        plt.plot(iterations, conv_cur_memory, label='Current Memory', color='#3498DB', linewidth=2.5, linestyle='-', marker='x', markersize=5, alpha=0.85)\n",
    "\n",
    "        # Labels and title\n",
    "        plt.xlabel(\"Iterations\", fontsize=12)\n",
    "        plt.ylabel(f\"Memory Usage ({memory_unit.upper()})\", fontsize=12)\n",
    "        plt.title(\"Memory Usage per Iteration\", fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', linewidth=0.6, alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Test estimate forward memory method\n",
    "# monitor = ModelMemoryMonitor(llama7b_model_name, use_amp=use_amp, device=device)\n",
    "# sample_inputs = monitor.simulate_input_ids(1)\n",
    "# monitor.test_forward_memory(sample_inputs, memory_unit='mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test estimate inference memory method\n",
    "# prompt = (\n",
    "#     'You should generate 100 words to explain diffusion model: '\n",
    "# )\n",
    "# monitor = ModelMemoryMonitor(llama7b_model_name, use_amp=use_amp, device=device)\n",
    "# prev_memory, peak_memory_lst, cur_memory_lst = monitor.test_iterative_inference_memory(prompt)\n",
    "# monitor.draw_memory_lines(prev_memory, peak_memory_lst, cur_memory_lst, memory_unit='gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training max peak memory: 10.76094102859497 (backward stage)\n",
      "the training memory consumption: {'model_loading': 2.050963878631592, 'forward_pass': 10.370193481445312, 'backward_pass': 4.446949005126953, 'optimize_model': 8.54787826538086}\n"
     ]
    }
   ],
   "source": [
    "# Test test training memory method\n",
    "monitor = ModelMemoryMonitor(tinyllama_model_name, use_amp=use_amp, device=device)\n",
    "peak_memory, memory_dict = monitor.test_training_memory(memory_unit='gb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-simulator-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
