{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = amp_supported = torch.cuda.is_available() and torch.cuda.get_device_capability(0) >= (7, 0)\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n",
    "# Load model directly\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\",   torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Memory and Max Memory Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Size: 2048\n",
      "EOS Token ID: 2\n",
      "Max Position Embeddings: 2048\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"Tinyllama/Tinyllama-1.1B-step-50K-105b\")\n",
    "\n",
    "hidden_size = config.hidden_size\n",
    "eos_token_id = config.eos_token_id\n",
    "max_position_embeddings = config.max_position_embeddings\n",
    "\n",
    "print(\"Hidden Size:\", hidden_size)\n",
    "print(\"EOS Token ID:\", eos_token_id)\n",
    "print(\"Max Position Embeddings:\", max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "b = 1 # batch size\n",
    "s = 1 # sequence length\n",
    "max_s = 4096 # max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelMemoryMonitor:\n",
    "    def __init__(self, model, tokenizer, batch_size=1, max_seq_len=4096, use_amp=False, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize the memory monitor for model inference.\n",
    "\n",
    "        Args:\n",
    "        model (torch.nn.Module): The preloaded model to monitor.\n",
    "        tokenizer (Tokenizer): Tokenizer to process input text.\n",
    "        batch_size (int): Number of samples in a batch.\n",
    "        max_seq_len (int): Maximum sequence length.\n",
    "        use_amp (bool): If True, enables mixed precision inference.\n",
    "        device (str): Device for inference (e.g., 'cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        self.use_amp = use_amp\n",
    "\n",
    "        # Set pad_token to eos_token if no padding token is available\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def bytes_to_mb(self, bytes: int) -> float:\n",
    "        \"\"\"Convert bytes to megabytes.\"\"\"\n",
    "        return bytes / 1048576\n",
    "\n",
    "    def bytes_to_gb(self, bytes: int) -> float:\n",
    "        \"\"\"Convert bytes to gigabytes.\"\"\"\n",
    "        return bytes / 1073741824\n",
    "    \n",
    "    def convert_memory(self, memory, memory_unit: Literal['byte', 'mb', 'gb']):\n",
    "        if memory_unit == 'mb':\n",
    "            return self.bytes_to_mb(memory)\n",
    "        elif memory_unit == 'gb':\n",
    "            return self.bytes_to_gb(memory)\n",
    "        return memory  # default to bytes\n",
    "\n",
    "\n",
    "    def simulate_input_ids(self, sequence_length: int, only_padding=False):\n",
    "        \"\"\"\n",
    "        Generate dummy input IDs based on a specified sequence length.\n",
    "\n",
    "        Args:\n",
    "        sequence_length (int): The sequence length for the dummy input.\n",
    "        only_padding (bool): If True, generates only padding tokens.\n",
    "        \n",
    "        Returns:\n",
    "        dict: A dictionary with input IDs and attention masks on the specified device.\n",
    "        \"\"\"\n",
    "        dummy_text = \"\" if only_padding else \" \".join([\"token\"] * int(sequence_length * 1.5))\n",
    "        inputs = self.tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_seq_len)\n",
    "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "\n",
    "        # Validate actual input length against expected sequence length\n",
    "        actual_length = inputs[\"input_ids\"].shape[1]\n",
    "        if actual_length != sequence_length:\n",
    "            print(f\"Warning: Expected sequence length ({sequence_length}) does not match actual input length ({actual_length}).\")\n",
    "        \n",
    "        # Check attention mask sum\n",
    "        attention_mask_sum = inputs[\"attention_mask\"].sum().item()\n",
    "        if attention_mask_sum != sequence_length:\n",
    "            print(f\"Warning: Attention mask sum ({attention_mask_sum}) does not match expected sequence length ({sequence_length}).\")\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    def estimate_forward_memory(self, sample_inputs, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "        \"\"\"\n",
    "        Estimate memory usage during inference.\n",
    "\n",
    "        Args:\n",
    "        sample_inputs (dict): Input data for the model.\n",
    "        memory_unit (Literal['byte', 'mb', 'gb']): Unit of memory measurement. Options are 'byte', 'mb', or 'gb'.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Previous memory, peak memory, and current memory in the specified unit.\n",
    "        \"\"\"\n",
    "        # Move the model to the specified device\n",
    "        self.model.cpu()\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Measure memory usage\n",
    "        prev_memory = torch.cuda.memory_allocated(self.device)\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # Run inference with optional AMP\n",
    "        with torch.no_grad():\n",
    "            if self.use_amp:\n",
    "                with torch.amp.autocast(device_type=str(self.device)):\n",
    "                    outputs = self.model(**sample_inputs)\n",
    "            else:\n",
    "                outputs = self.model(**sample_inputs)\n",
    "\n",
    "        # Calculate memory usage\n",
    "        peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "        cur_memory = torch.cuda.memory_allocated(self.device)\n",
    "\n",
    "        # Convert memory usage to the specified unit\n",
    "        if memory_unit == 'mb':\n",
    "            prev_memory, peak_memory, cur_memory = map(self.bytes_to_mb, [prev_memory, peak_memory, cur_memory])\n",
    "        elif memory_unit == 'gb':\n",
    "            prev_memory, peak_memory, cur_memory = map(self.bytes_to_gb, [prev_memory, peak_memory, cur_memory])\n",
    "\n",
    "        # Print memory usage summary\n",
    "        print(f\"Previous Memory: {prev_memory} {memory_unit.upper()}; Peak Memory: {peak_memory} {memory_unit.upper()}; Current Memory: {cur_memory} {memory_unit.upper()}\")\n",
    "        print(f\"Peak Memory Difference: {peak_memory - prev_memory} {memory_unit.upper()}\")\n",
    "        print(f\"Total Memory Consumption: {cur_memory - prev_memory} {memory_unit.upper()}\")\n",
    "\n",
    "        return prev_memory, peak_memory, cur_memory\n",
    "    \n",
    "    def estimate_inference_memory(self, prompt, max_iters = 100, memory_unit: Literal['byte', 'mb', 'gb'] = 'byte'):\n",
    "        self.model.cpu()\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        prev_memory = torch.cuda.memory_allocated(self.device)\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "        \n",
    "        peak_memory_lst = []\n",
    "        cur_memory_lst = []\n",
    "\n",
    "        for i in range(max_iters):\n",
    "            # Reset peak memory stats for each token generation\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.use_amp:\n",
    "                    with torch.amp.autocast(device_type=self.device):\n",
    "                        outputs = self.model.generate(\n",
    "                            input_ids,\n",
    "                            max_length=input_ids.shape[1] + 1,\n",
    "                            eos_token_id=self.tokenizer.eos_token_id,\n",
    "                            pad_token_id=self.tokenizer.eos_token_id,\n",
    "                            do_sample=False  # Disable sampling for deterministic output\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids,\n",
    "                        max_length=input_ids.shape[1] + 1,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        do_sample=False\n",
    "                    )\n",
    "                next_token_id = outputs[:, -1:]\n",
    "\n",
    "                generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "                \n",
    "                if next_token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    print(f\"EOS token generated at iteration {i+1}\")\n",
    "                    break\n",
    "\n",
    "                peak_memory = torch.cuda.max_memory_allocated(self.device)\n",
    "                cur_memory = torch.cuda.memory_allocated(self.device)\n",
    "                peak_memory_lst = peak_memory_lst.append(peak_memory)\n",
    "                cur_memory_lst = cur_memory_lst.append(cur_memory)\n",
    "                \n",
    "        return prev_memory, peak_memory_lst, cur_memory_lst\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Expected sequence length (1) does not match actual input length (2).\n",
      "Warning: Attention mask sum (2) does not match expected sequence length (1).\n",
      "Previous Memory: 4208.833984375 MB; Peak Memory: 4209.0068359375 MB; Current Memory: 4208.9990234375 MB\n",
      "Peak Memory Difference: 0.1728515625 MB\n",
      "Total Memory Consumption: 0.1650390625 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4208.833984375, 4209.0068359375, 4208.9990234375)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = ModelMemoryMonitor(model, tokenizer, use_amp=use_amp, device=device)\n",
    "sample_inputs = monitor.simulate_input_ids(1)\n",
    "monitor.estimate_inference_memory(sample_inputs, memory_unit='mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    if use_amp:\n",
    "        with torch.amp.autocast(device_type=str(device)):\n",
    "            outputs = model(**sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The TinyLlama project aims to pretrain 10,425 volunteer dog walkers and the public in how to use their dogs effectively so that they can be an integral part of communities.\n",
      "In total over three years with more than 768 miles covered, there are now approximately one million animals walking safely through these villages – which is about as many for every person who walks on average each day. They have seen improvements such as reduced traffic accidents resulting from people using them instead of cars or bicycles by making it easier for other road users to find ways around busy roads; improved healthcare due to increased numbers being able access veterinary services without leaving local hospitals when seeking treatment; improved social interaction between residents living at home versus driving themselves because drivers tend not take into consideration all those pesky pedestrians passing near where you're standing! These positive changes also mean we will see even higher usage rates during school terms this year compared to last term (and next season too!) So if anyone has ideas of projects aimed here but isn't sure what your community needs then do get involved - contact us directly: [email protected] If any further information regarding our work would like anything else clarified or added please email hello@dogwalkinginwales.co.uk This site uses cookies to ensure its best experience possible. Please read our Privacy Policy. You accept the policy after signing up? Q: How can i pass parameters dynamically inside function I am having trouble doing something similar to this question asked here https://groups.google.com/forum/#!topic/php-netbeans3/ZxYWJq9fyvk :  \"Can I set variables globally within functions via XML syntax?\"\n",
      "                             <script src=\"/js/jquery-ui-latest.min.js\"></head>` W3C HTML Publication Standard v3.8 Final Version\">     <!-- Generated code -->         <?= $this->render('_header'); ?>            <body><p id='msgbox'>Message:<br class=\"message\"><b></div>      \n",
      "        var xmlData={};\n",
      "$('#txt').val($(\"#input\").text()); //This\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input prompt\n",
    "prompt = (\n",
    "    'The TinyLlama project aims to pretrain '\n",
    ")\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# Generation settings\n",
    "max_length = 500\n",
    "top_k = 10\n",
    "repetition_penalty = 1.5\n",
    "\n",
    "# Run inference with temperature sampling and repetition penalty\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f\"Result: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The TinyLlama project aims to pretrain 10,425 volunteer dog walkers and the public in how to use their dogs effectively so that they can be an integral part of communities.\\nIn total over three years with more than 768 miles covered, there are now approximately one million animals walking safely through these villages – which is about as many for every person who walks on average each day. They have seen improvements such as reduced traffic accidents resulting from people using them instead of cars or bicycles by making it easier for other road users to find ways around busy roads; improved healthcare due to increased numbers being able access veterinary services without leaving local hospitals when seeking treatment; improved social interaction between residents living at home versus driving themselves because drivers tend not take into consideration all those pesky pedestrians passing near where you\\'re standing! These positive changes also mean we will see even higher usage rates during school terms this year compared to last term (and next season too!) So if anyone has ideas of projects aimed here but isn\\'t sure what your community needs then do get involved - contact us directly: [email protected] If any further information regarding our work would like anything else clarified or added please email hello@dogwalkinginwales.co.uk This site uses cookies to ensure its best experience possible. Please read our Privacy Policy. You accept the policy after signing up? Q: How can i pass parameters dynamically inside function I am having trouble doing something similar to this question asked here https://groups.google.com/forum/#!topic/php-netbeans3/ZxYWJq9fyvk :  \"Can I set variables globally within functions via XML syntax?\"\\n<?xml version=\"1.0\" encoding = \\'UTF-8\"?>   <html>    <!DOCTYPE html PUBLIC \"-// W3C HTML Publication Standard v3.8 Final Version\">     <!-- Generated code -->         <?= $this->render(\\'_header\\'); ?>            <body><p id=\\'msgbox\\'>Message:<br class=\"message\"><b></div>      \\r       </form>\\r           \\r                             <script src=\"/js/jquery-ui-latest.min.js\"></head>`\\r\\n        var xmlData={};\\r\\n$(\\'#txt\\').val($(\"#input\").text()); //This'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-simulator-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
